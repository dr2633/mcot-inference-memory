{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GSM8K Demo: Chain-of-Thought (CoT) vs. Memory-Augmented CoT (mCoT)\n",
    "This notebook runs **baseline CoT reasoning** and **memory-augmented CoT (mCoT)** on the **GSM8K math dataset** using **Qwen**."
   ],
   "id": "862e2a2f3c67e621"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T05:30:27.923877Z",
     "start_time": "2025-02-26T05:30:17.013216Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install torch transformers datasets faiss-cpu sentence-transformers",
   "id": "4f48572564c60044",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.4.1)\r\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.40.1)\r\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.0.1)\r\n",
      "Collecting faiss-cpu\r\n",
      "  Downloading faiss_cpu-1.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.4 kB)\r\n",
      "Collecting sentence-transformers\r\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.24.6)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\r\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\r\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\r\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\r\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.9.5)\r\n",
      "Collecting transformers\r\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.4.2)\r\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.12.0)\r\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\r\n",
      "Collecting huggingface-hub>=0.22.0 (from datasets)\r\n",
      "  Using cached huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\r\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n",
      "Downloading faiss_cpu-1.10.0-cp312-cp312-macosx_11_0_arm64.whl (3.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.2/3.2 MB\u001B[0m \u001B[31m13.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m275.9/275.9 kB\u001B[0m \u001B[31m9.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\r\n",
      "Using cached huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\r\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-macosx_12_0_arm64.whl (30.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.7/30.7 MB\u001B[0m \u001B[31m18.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\r\n",
      "Installing collected packages: pyarrow, faiss-cpu, huggingface-hub, tokenizers, transformers, sentence-transformers\r\n",
      "  Attempting uninstall: pyarrow\r\n",
      "    Found existing installation: pyarrow 14.0.2\r\n",
      "    Uninstalling pyarrow-14.0.2:\r\n",
      "      Successfully uninstalled pyarrow-14.0.2\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface_hub 0.24.6\r\n",
      "    Uninstalling huggingface_hub-0.24.6:\r\n",
      "      Successfully uninstalled huggingface_hub-0.24.6\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.19.1\r\n",
      "    Uninstalling tokenizers-0.19.1:\r\n",
      "      Successfully uninstalled tokenizers-0.19.1\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.40.1\r\n",
      "    Uninstalling transformers-4.40.1:\r\n",
      "      Successfully uninstalled transformers-4.40.1\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "optimum 1.19.1 requires transformers[sentencepiece]<4.41.0,>=4.26.0, but you have transformers 4.49.0 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed faiss-cpu-1.10.0 huggingface-hub-0.29.1 pyarrow-17.0.0 sentence-transformers-3.4.1 tokenizers-0.21.0 transformers-4.49.0\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2: Load the GSM8K Dataset\n",
    "We use the **train split** to evaluate our reasoning methods.\n"
   ],
   "id": "fd99e04acb8d0ec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load GSM8K\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "gsm8k_samples = dataset[\"train\"].select(range(10))  # Load only 10 samples for quick testing\n",
    "\n",
    "print(f\"Loaded {len(gsm8k_samples)} samples from GSM8K.\")"
   ],
   "id": "8a580229e74c96a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3: Load Qwen Model & Tokenizer\n",
    "We use **Qwen/Qwen2-7B-Instruct** for our reasoning tasks.\n"
   ],
   "id": "641fdbcbcc1eb4d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load Qwen Model\n",
    "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ],
   "id": "e47b6e88bb36129f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 4: Define Chain-of-Thought (CoT) Prompting Function\n",
    "We construct a **step-by-step reasoning prompt** and generate outputs.\n"
   ],
   "id": "c03577686f248cd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T05:31:42.454919Z",
     "start_time": "2025-02-26T05:31:42.451530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_cot_answer(question, prompt_prefix=\"Let's think step by step.\", max_new_tokens=256):\n",
    "    \"\"\"Generates a Chain-of-Thought answer for a given question.\"\"\"\n",
    "    prompt = f\"Question: {question}\\n{prompt_prefix}\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ],
   "id": "752d16da1a43a02a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5: Run Baseline CoT Reasoning\n",
    "We generate step-by-step solutions using standard Chain-of-Thought prompting.\n"
   ],
   "id": "f3e0d1d5b497a00e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for idx, sample in enumerate(gsm8k_samples):\n",
    "    question = sample[\"question\"]\n",
    "    gold_answer = sample[\"answer\"]\n",
    "    generated_answer = generate_cot_answer(question)\n",
    "    \n",
    "    print(f\"\\nQuestion {idx+1}: {question}\")\n",
    "    print(f\"Model Answer: {generated_answer}\")\n",
    "    print(f\"Gold Answer: {gold_answer}\")"
   ],
   "id": "8b86fd5b3082c675"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 6: Integrate Memory Retrieval (mCoT)\n",
    "Now, we retrieve relevant past solutions using FAISS.\n"
   ],
   "id": "64a4405426aad84a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T05:33:08.866882Z",
     "start_time": "2025-02-26T05:32:57.093359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from memory.retrieval_faiss import FAISSRetriever\n",
    "\n",
    "# Initialize FAISS memory retrieval\n",
    "retriever = FAISSRetriever(index_path=\"faiss_index\")\n",
    "retriever.load_index()"
   ],
   "id": "bc8edd27735e8614",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nmodule 'torch.library' has no attribute 'register_fake'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1863\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1862\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m   1864\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m     89\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap\u001B[38;5;241m.\u001B[39m_gcd_import(name[level:], package, level)\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1387\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1331\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:935\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:995\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:488\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:53\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegrations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msdpa_attention\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sdpa_attention_forward\n\u001B[0;32m---> 53\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LOSS_MAPPING\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpytorch_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[1;32m     55\u001B[0m     Conv1D,\n\u001B[1;32m     56\u001B[0m     apply_chunking_to_forward,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     62\u001B[0m     translate_to_torch_parallel_style,\n\u001B[1;32m     63\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/loss/loss_utils.py:19\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BCEWithLogitsLoss, MSELoss\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss_deformable_detr\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss_for_object_detection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/loss/loss_deformable_detr.py:4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_transforms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m center_to_corners_format\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_scipy_available\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/image_transforms.py:22\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     23\u001B[0m     ChannelDimension,\n\u001B[1;32m     24\u001B[0m     ImageInput,\n\u001B[1;32m     25\u001B[0m     get_channel_dimension_axis,\n\u001B[1;32m     26\u001B[0m     get_image_size,\n\u001B[1;32m     27\u001B[0m     infer_channel_dimension_format,\n\u001B[1;32m     28\u001B[0m )\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExplicitEnum, TensorType, is_jax_tensor, is_tf_tensor, is_torch_tensor\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/image_utils.py:65\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torchvision_available():\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m io \u001B[38;5;28;01mas\u001B[39;00m torchvision_io\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InterpolationMode\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/__init__.py:10\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mextension\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/_meta_registrations.py:163\u001B[0m\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m grad\u001B[38;5;241m.\u001B[39mnew_empty((batch_size, channels, height, width))\n\u001B[0;32m--> 163\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39mlibrary\u001B[38;5;241m.\u001B[39mregister_fake(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorchvision::nms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmeta_nms\u001B[39m(dets, scores, iou_threshold):\n\u001B[1;32m    165\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_check(dets\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m, \u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboxes should be a 2d tensor, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdets\u001B[38;5;241m.\u001B[39mdim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mD\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'torch.library' has no attribute 'register_fake'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1863\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1862\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m   1864\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m     89\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap\u001B[38;5;241m.\u001B[39m_gcd_import(name[level:], package, level)\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1387\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1331\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:935\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:995\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:488\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/integrations/integration_utils.py:36\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpackaging\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m\n\u001B[0;32m---> 36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PreTrainedModel, TFPreTrainedModel\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__ \u001B[38;5;28;01mas\u001B[39;00m version\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1412\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[0;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1851\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m-> 1851\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module[name])\n\u001B[1;32m   1852\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1865\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1864\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1865\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1866\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1867\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1868\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nmodule 'torch.library' has no attribute 'register_fake'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmemory\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mretrieval_faiss\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FAISSRetriever\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Initialize FAISS memory retrieval\u001B[39;00m\n\u001B[1;32m      4\u001B[0m retriever \u001B[38;5;241m=\u001B[39m FAISSRetriever(index_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfaiss_index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/GitHub/inference-preference-optimization/memory/retrieval_faiss.py:6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Configure logging\u001B[39;00m\n\u001B[1;32m      9\u001B[0m logging\u001B[38;5;241m.\u001B[39mbasicConfig(level\u001B[38;5;241m=\u001B[39mlogging\u001B[38;5;241m.\u001B[39mINFO, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%(asctime)s\u001B[39;00m\u001B[38;5;124m [\u001B[39m\u001B[38;5;132;01m%(levelname)s\u001B[39;00m\u001B[38;5;124m] \u001B[39m\u001B[38;5;132;01m%(message)s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/__init__.py:14\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     10\u001B[0m     export_dynamic_quantized_onnx_model,\n\u001B[1;32m     11\u001B[0m     export_optimized_onnx_model,\n\u001B[1;32m     12\u001B[0m     export_static_quantized_openvino_model,\n\u001B[1;32m     13\u001B[0m )\n\u001B[0;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_encoder\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ParallelSentencesDataset, SentencesDataset\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mLoggingHandler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoggingHandler\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/__init__.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m annotations\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[1;32m      5\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCrossEncoder\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:18\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenization_utils_base\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BatchEncoding\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PushToHubMixin\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluation\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mSentenceEvaluator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceEvaluator\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreaders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InputExample\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mSentenceTransformer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/evaluation/__init__.py:9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mMSEEvaluator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MSEEvaluator\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mMSEEvaluatorFromDataFrame\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MSEEvaluatorFromDataFrame\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mNanoBEIREvaluator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NanoBEIREvaluator\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mParaphraseMiningEvaluator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ParaphraseMiningEvaluator\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mRerankingEvaluator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RerankingEvaluator\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/evaluation/NanoBEIREvaluator.py:11\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tensor\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluation\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mInformationRetrievalEvaluator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InformationRetrievalEvaluator\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluation\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mSentenceEvaluator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceEvaluator\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:33\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_torch_npu_available\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdynamic_module_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_class_from_dynamic_module, get_relative_import_files\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_card\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformerModelCardData, generate_model_card\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msimilarity_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SimilarityFunction\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __MODEL_HUB_ORGANIZATION__, __version__\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/model_card.py:25\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautonotebook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TrainerCallback\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegrations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CodeCarbonCallback\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodelcard\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m make_markdown_table\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtrainer_callback\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TrainerControl, TrainerState\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1412\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[0;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1851\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1849\u001B[0m     value \u001B[38;5;241m=\u001B[39m Placeholder\n\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m-> 1851\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module[name])\n\u001B[1;32m   1852\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[1;32m   1853\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1865\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[0;34m(self, module_name)\u001B[0m\n\u001B[1;32m   1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m   1864\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1865\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1866\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1867\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1868\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nmodule 'torch.library' has no attribute 'register_fake'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 7: Run Memory-Augmented CoT (mCoT)\n",
    "The model retrieves relevant past reasoning before generating new outputs.\n"
   ],
   "id": "595aa16c139a477d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for idx, sample in enumerate(gsm8k_samples):\n",
    "    question = sample[\"question\"]\n",
    "    retrieved_memories = retriever.retrieve_memory(question, top_k=3)\n",
    "    retrieved_text = \"\\n\".join([m[\"text\"] for m in retrieved_memories]) if retrieved_memories else \"\"\n",
    "    \n",
    "    # Combine past memories with new question\n",
    "    memory_augmented_prompt = f\"Previous Reasoning:\\n{retrieved_text}\\n\\nNew Question:\\n{question}\\nLet's think step by step.\"\n",
    "    \n",
    "    generated_answer = generate_cot_answer(memory_augmented_prompt)\n",
    "    \n",
    "    print(f\"\\nQuestion {idx+1}: {question}\")\n",
    "    print(f\"Retrieved Memory: {retrieved_text}\")\n",
    "    print(f\"mCoT Answer: {generated_answer}\")"
   ],
   "id": "e8413ad400cc3582"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 8: Compare CoT vs. mCoT Performance\n",
    "Now, we analyze accuracy, efficiency, and consistency.\n"
   ],
   "id": "a57d269478960853"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T05:35:31.462919Z",
     "start_time": "2025-02-26T05:35:31.461258Z"
    }
   },
   "cell_type": "code",
   "source": "## Add evals and logging results ",
   "id": "521b616afe1129d7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7b9aabacc21e34c7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
